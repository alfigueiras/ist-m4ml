{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 - tem que ser assim para assegurar que tem cuda\n",
    "import numpy as np \n",
    "import numpy.random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset,DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders on kaggle to save outputs\n",
    "# os.makedirs('/kaggle/working/models')\n",
    "# os.makedirs('/kaggle/working/models/DDPM1_IMP')\n",
    "# os.makedirs('/kaggle/working/results')\n",
    "# os.makedirs('/kaggle/working/results/DDPM1_IMP')\n",
    "# os.makedirs('/kaggle/working/extras')\n",
    "# os.makedirs('/kaggle/working/extras/DDPM1_IMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=500\n",
    "IMG_SIZE=128\n",
    "DEVICE=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Linear noise variance schedule\n",
    "def linear_noise_schedule(start=0.0001, end=0.02, steps=T):\n",
    "    return torch.linspace(start=start, end=end, steps=steps)\n",
    "\n",
    "betas=linear_noise_schedule().to(DEVICE)\n",
    "alphas=1-betas\n",
    "\n",
    "print(alphas)\n",
    "alphas_cumprod=torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod=torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod=torch.sqrt(1-alphas_cumprod)\n",
    "\n",
    "#Forward diffusion used just for the example\n",
    "def forward_diffusion_process(x0, t):\n",
    "    \"\"\"\n",
    "    Get noisy image of x0 obtained after t timesteps\n",
    "    \"\"\"\n",
    "    noise=rand.standard_normal(size=x0.shape)\n",
    "    return sqrt_alphas_cumprod[t].cpu()*x0+sqrt_one_minus_alphas_cumprod[t].cpu()*noise, noise\n",
    "\n",
    "# Can produce noise for multiple images at the same time, used for training\n",
    "def get_noisy_images(x, t):\n",
    "    sqrt_alpha_hat = torch.sqrt(alphas_cumprod[t])[:, None, None, None]\n",
    "    sqrt_one_minus_alpha_hat = torch.sqrt(1 - alphas_cumprod[t])[:, None, None, None]\n",
    "    noise = torch.randn_like(x)\n",
    "    return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise, noise\n",
    "\n",
    "# Sample a set of t's with size=num\n",
    "def sample_timesteps(num):\n",
    "    return torch.tensor(rand.choice(range(T),size=num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces image resolution\n",
    "def transform_img(image : Image):\n",
    "    \"\"\"\n",
    "    Horizontal flips given image with random probability\n",
    "    and scales it to values between -1 and 1.\n",
    "    \"\"\"\n",
    "    image.thumbnail((IMG_SIZE,IMG_SIZE))\n",
    "    prob=rand.random()\n",
    "\n",
    "    if prob>0.5:\n",
    "        image=image.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    img_arr=np.array(image).astype(np.float32)\n",
    "\n",
    "    #Scale between -1 and 1\n",
    "    img_arr=2*img_arr/255-1\n",
    "\n",
    "    return img_arr\n",
    "\n",
    "def reverse_transform_img(img_arr):\n",
    "    \"\"\"\n",
    "    Given an image array scales it back to valid RGB\n",
    "    \"\"\"\n",
    "    img_arr=(img_arr+1)/2\n",
    "    img_arr*=255\n",
    "    img_arr=np.array(img_arr).astype(np.uint8)\n",
    "    return img_arr\n",
    "\n",
    "#Remove this when on kaggle ->\n",
    "\n",
    "im=Image.open(\"test_img.jpg\")\n",
    "img=transform_img(im)\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for t in range(0, T, stepsize):\n",
    "    plt.subplot(1, num_images+1, int(t/stepsize) + 1)\n",
    "    noisy_img, noise = forward_diffusion_process(img, t)\n",
    "    noisy_img=reverse_transform_img(noisy_img)\n",
    "    plt.imshow(noisy_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def load_transformed_dataset():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), # Scales data into [0,1]\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    cifar = torchvision.datasets.CIFAR10(root=\".\", download=True,\n",
    "                                         transform=data_transform)\n",
    "    return cifar\n",
    "\n",
    "def reverse_transform_tensor(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: t.clamp(-1,1)),\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    \n",
    "    return reverse_transforms(image)\n",
    "\n",
    "data = load_transformed_dataset()\n",
    "\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "print(data.class_to_idx)\n",
    "\n",
    "#Dataloader for only 1 class: automobiles\n",
    "cars = Subset(data, [i for i, (x, y) in enumerate(data) if y == 1])\n",
    "\n",
    "dataloader_cars = DataLoader(cars, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simulate forward diffusion\n",
    "image = next(iter(dataloader))[0]\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n",
    "\n",
    "    img, noise = forward_diffusion_process(image, t)\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(img.shape) == 4:\n",
    "        img = img[0, :, :, :]\n",
    "    plt.imshow(reverse_transform_tensor(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used in the encoder, bottleneck and decoder of the UNet\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "        \n",
    "# Used in the encoder part of the UNet\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    #Uses positional embeddings\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "# Used in the decoder part of the UNet\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    #Uses both positional embeddings and skip connections\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "#Self Attention module used troughout the network, after downsample and upsample blocks\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels        \n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-1]\n",
    "        x = x.view(-1, self.channels, size * size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, size, size)\n",
    "\n",
    "class SimplifiedUNet(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "        self.start_dconv = DoubleConv(c_in, 64)\n",
    "        self.downsample1 = DownsampleBlock(64, 128)\n",
    "        self.sattention1 = SelfAttention(128)\n",
    "        self.downsample2 = DownsampleBlock(128, 256)\n",
    "        self.sattention2 = SelfAttention(256)\n",
    "        self.downsample3 = DownsampleBlock(256, 256)\n",
    "        self.sattention3 = SelfAttention(256)\n",
    "\n",
    "        self.bottleneck_dconv1 = DoubleConv(256, 512)\n",
    "        self.bottleneck_dconv2 = DoubleConv(512, 512)\n",
    "        self.bottleneck_dconv3 = DoubleConv(512, 256)\n",
    "\n",
    "        self.upsample1 = UpsampleBlock(512, 128)\n",
    "        self.sattention4 = SelfAttention(128)\n",
    "        self.upsample2 = UpsampleBlock(256, 64)\n",
    "        self.sattention5 = SelfAttention(64)\n",
    "        self.upsample3 = UpsampleBlock(128, 64)\n",
    "        self.sattention6 = SelfAttention(64)\n",
    "        self.end_conv = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        \"\"\"\n",
    "        Adapted positional encoding implementation from PyTorch docs\n",
    "        \"\"\"\n",
    "        div_term = torch.exp(torch.arange(0, channels, 2) * (-math.log(10000.0) / channels)).to(self.device)\n",
    "        pos_enc = torch.zeros(t.shape[0], channels, device=self.device)\n",
    "        pos_enc[:, 0::2] = torch.sin(t * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(t * div_term)\n",
    "        \n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_emb_dim)[:x.size(0)]\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.start_dconv(x)\n",
    "        x2 = self.downsample1(x1, t)\n",
    "        x2 = self.sattention1(x2)\n",
    "        x3 = self.downsample2(x2, t)\n",
    "        x3 = self.sattention2(x3)\n",
    "        x4 = self.downsample3(x3, t)\n",
    "        x4 = self.sattention3(x4)\n",
    "\n",
    "        # Bottleneck\n",
    "        x4 = self.bottleneck_dconv1(x4)\n",
    "        x4 = self.bottleneck_dconv2(x4)\n",
    "        x4 = self.bottleneck_dconv3(x4)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upsample1(x4, x3, t)\n",
    "        x = self.sattention4(x)\n",
    "        x = self.upsample2(x, x2, t)\n",
    "        x = self.sattention5(x)\n",
    "        x = self.upsample3(x, x1, t)\n",
    "        x = self.sattention6(x)\n",
    "        output = self.end_conv(x)\n",
    "        return output\n",
    "    \n",
    "model = SimplifiedUNet(c_in=3, c_out=3)\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, n):\n",
    "    \"\"\"\n",
    "    Implementation of the algorithm 2 of the paper (Sampling algorithm)\n",
    "    \"\"\"\n",
    "    print(f\"Sampling {n} new images\")\n",
    "\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    t_list=list(range(1,T,stepsize))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Creates random noise image\n",
    "        x = torch.randn((n, 3, IMG_SIZE, IMG_SIZE)).to(DEVICE)\n",
    "        for i in list(range(1,T))[::-1]:\n",
    "            t=(torch.ones(n) * i).long().to(DEVICE)\n",
    "\n",
    "            # Predicts noise at timestep t for all noisy images in x\n",
    "            predicted_noise=model(x,t)\n",
    "\n",
    "            alpha=alphas[t][:, None, None, None] \n",
    "            alpha_cumprod=alphas_cumprod[t][:, None, None, None]\n",
    "            beta=betas[t][:, None, None, None]\n",
    "\n",
    "            if i>1:\n",
    "                noise=torch.randn_like(x)\n",
    "            else:\n",
    "                noise=torch.zeros_like(x)\n",
    "\n",
    "            # Removing noise from the image\n",
    "            x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_cumprod))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "            print(x)\n",
    "            #Backward process figure\n",
    "            if i in t_list:\n",
    "                if len(x.shape) == 4:\n",
    "                    img = x[0, :, :, :].cpu()\n",
    "                plt.subplot(1, num_images+1, int(i/stepsize) + 1)\n",
    "                new_img=reverse_transform_tensor(img)\n",
    "                plt.imshow(new_img)\n",
    "\n",
    "    plt.show()\n",
    "    model.train()\n",
    "    \n",
    "    # Transform to deafult color intensity values\n",
    "    x=(x.clamp(-1,1)+1)/2\n",
    "    x=(x*255).type(torch.uint8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, path, **kwargs):\n",
    "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, dataloader, lr=0.001, device='cuda', name='DDPM1', qSample=True):\n",
    "    model = SimplifiedUNet().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    mse = nn.MSELoss()\n",
    "    losses=[]\n",
    "    epoch_times=[]\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting epoch {epoch}:\")\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, (images, classes) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            t = sample_timesteps(len(images)).to(device)\n",
    "            x_t, noise = get_noisy_images(images, t)\n",
    "            predicted_noise = model(x_t, t)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "\n",
    "            if i==len(pbar)-1:\n",
    "                epoch_times.append(pbar.format_dict[\"elapsed\"])\n",
    "            \n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item()} \")\n",
    "        if qSample or (epoch+1)%50==0 or epoch==0:\n",
    "            sampled_images = sample(model, n=64)\n",
    "            save_images(sampled_images, os.path.join(\"results\", name, f\"{epoch+1}.jpg\"))\n",
    "            torch.save(model.state_dict(), os.path.join(\"models\", name, f\"checkpoint{epoch+1}.pt\"))\n",
    "            np.save(os.path.join(\"extras\", name, f\"losses.npy\"), np.array(losses))\n",
    "            np.save(os.path.join(\"extras\", name, f\"epoch_times.npy\"), np.array(epoch_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(500, dataloader_cars, qSample=False, name=\"DDPM1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SimplifiedUNet()\n",
    "model.load_state_dict(torch.load(os.path.join(\"models\", \"DDPM1\", f\"checkpoint1.pt\")))\n",
    "model.to(DEVICE)\n",
    "sample(model,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
